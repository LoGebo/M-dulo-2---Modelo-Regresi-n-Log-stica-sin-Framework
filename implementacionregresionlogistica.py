# -*- coding: utf-8 -*-
"""implementacionRegresionLogistica.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VSpbLm85QsuAF7KGRLh19qEN6NF1nFET

## Jesús Daniel Martínez García A00833591
"""


import pandas as pd

# dataset de kaggle https://www.kaggle.com/datasets/vikramamin/bank-loan-approval-lr-dt-rf-and-auc
df = pd.read_csv('Cancer_Data.csv')
df

#Convertimos  variable categorica para que el modelo pueda manejarla.
df['diagnosis'] = df['diagnosis'].replace({'B': 0, 'M': 1})

df_x = df[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',
           'compactness_mean', 'concavity_mean', 'concave points_mean']]

## Nuestra variable a predecir con la regresión logística será el diagnóstico (0 = Benigno, 1 = Maligno)
df_y = df['diagnosis']

## Aquí utilizaremos la función de train_test_split como la utilizada en la actividad de regresión logistica multiclase para separar nuestro datasets en datos de entrenamiento y prueba
##la función divide los datos en 80% entrenamiento y 20% prueba.

from sklearn.model_selection import train_test_split

#Esto nos servirá para ver si el modelo generaliza bien con otros datos

X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=42)

## Escalar las features nos permitira que el modelo trate a todas las features con igual de importancia

from sklearn.preprocessing import StandardScaler


sc = StandardScaler()

X_train_scaled = sc.fit_transform(X_train)


#transform utilizda en los datos de prueba
X_test_scaled = sc.transform(X_test)

##inicializacion de theta y columna de 1sss

import numpy as np

# Inicialización del vector theta con valores random (incluyendo el término dell bias)


theta = np.random.randn(len(X_train_scaled[0]) + 1, 1)

# Añadir la columna de unos a X_train_scaled para incluir el bias en X_vect
X_vect = np.c_[np.ones((len(X_train_scaled), 1)), X_train_scaled]
X_test_vect = np.c_[np.ones((len(X_test_scaled), 1)), X_test_scaled]


print(X_vect[:5])
print(X_vect.shape)

##DEFINIMOS LAS FUNCIONES NECESARIAS PARA IMEPLEMENTAR LA REGRESIÓN LOGÍSTICA

import matplotlib.pyplot as plt


##funcion sigmoide convierte en un num entre 0 y 1
def sigmoid_function(X):
    return 1 / (1 + np.exp(-X))



##Regresión logística, utiliza gradient descent para ajustar las thetas, x son los features, y la label,
#alpha el learning reate y los epochs la cantidad de veces que se actulizarán los parametros
def log_regression(X, y, theta, alpha, epochs):
  y_ = np.reshape(y, (len(y), 1)) # shape (150,1)
  N = len(X)
  avg_loss_list = []
  for epoch in range(epochs):
    sigmoid_x_theta = sigmoid_function(X_vect.dot(theta)) # shape: (150,5).(5,1) = (150,1)
    grad = (1/N) * X_vect.T.dot(sigmoid_x_theta - y_) # shapes: (5,150).(150,1) = (5, 1)
    theta = theta - (alpha * grad)
    hyp = sigmoid_function(X_vect.dot(theta)) # shape (150,5).(5,1) = (150,1)
    avg_loss = -np.sum(np.dot(y_.T, np.log(hyp) + np.dot((1-y_).T, np.log(1-hyp)))) / len(hyp)
    if epoch % 1000 == 0:
      print('epoch: {} | avg_loss: {}'.format(epoch, avg_loss))

    avg_loss_list.append(avg_loss)
  plt.plot(np.arange(1, epochs), avg_loss_list[1:], color='red')
  plt.title('Cost function')
  plt.xlabel('Epochs')
  plt.ylabel('Cost')
  plt.show()
  return theta

# Entrenar el modelo

alpha = 0.01
epochs = 10000

theta_final = log_regression(X_vect, y_train, theta, alpha, epochs)

def predict(X, theta):
    probabilities = sigmoid_function(np.dot(X, theta))
    return probabilities >= 0.5  # Si la probabilidad es mayor o igual a 0.5, predice 1, sino nel 0



# Asegurarnos de q theta_final tiene el tipo de dato correcto
theta_final = theta_final.astype(float)

# Hacer predicciones con el conjunto de prueba
y_pred_test = predict(X_test_vect, theta_final)

# Convertir las predicciones booleanas en 0 y 1 para calcular las métricas
y_pred_test = y_pred_test.astype(int)

# Aplanar los arrays si tibnes multidimension
y_test = np.array(y_test).ravel()
y_pred_test = np.array(y_pred_test).ravel()

# Calcular la precisión manualmente
accuracy_manual = np.mean(y_test == y_pred_test)
print(f'Precisión del modelo: {accuracy_manual * 100:.2f}%')



# Usa pandas para generar la matriz de confusión
conf_matrix = pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred_test, name='Predicted'))


print(conf_matrix)


accuracy = np.mean(y_test == y_pred_test)


print(f'Exactitud calculada.   : {accuracy * 100:.2f}%')

"""##Reporte.

Para el dataset de cáncer data buscamos predecir el label de diagnosis donde m representaba cáncer maligno y b cáncer benigno. Se implementa el modelo de regresión logística al ser un algoritmo efectivo para el problema que representa, viendo que es una clasificación binaria.

Primero se cargan los datos utilizando pandas, se separan las features y labels así como transformar la label categórica a numérica con 0 para benigno y 1 para maligno, lo que facilitaría el uso en el modelo.

Después dividimos los datos de entrenamiento: 80% training y 20% prueba. Esto lo hacemos para evaluar el modelo con datos que no ha visto antes y validar si generaliza o no.

 Se aplicó un standar scaler, que básicamente asegura que todas las features tengan el mismo peso en el entrenamiento y para el de prueba se escalan usando lo mismo que se calculó en los de entrenamiento, uno con fit_transform y el otro solo con transform.

 En las funciones relevantes para la regresión logística se definió la sigmoide y la función para entrenar el modelo utilizando el gradient descent que ajusta las thetas, alpha sería el learning rate, que contra que tran rapido se ajustan las thetas en cada iteración, después los epochs que es la cantidad de veces que se van a actualizar los parametros del modelo.

 Se crea una función predict que utiliza la funcion sigmoide que calcule las probabilidades de que un diagnotico sea maligno (1), si es mayor o igual a 0.5, si es menor, predice benigno (0).

 Se hacen las predicciones sobre los datos de test y se comparan. El accuracy ahí nos da cuántas veces el modelo predijo correctamente, en este caso obtuvo un 96.4%. También, se implementó una matriz de confusión que muestra cómo se comportó el modelo con más detalle: verdaderos negativos (70), es decir, que el modelo predijo bien que 70 personas no tenían cáncer maligno. Falsos positivos (1) el modelo solo predijo mal que una persona tenía cáncer maligno, cuando no. Verdaderos positivos (40), el modelo predijo correctamente que esa cantidad tenía cáncer maligno. Falsos negativos (3), el modelo dio mal que 3 personas no tenían cáncer maligno, cuando sí.

En cuanto a los hiperparámetros, se decidió utilizar la función de grid search, específicamente para ajustar el learning rate y los epochs, durante estas pruebas aunque la función si identificaba una combinación como la mejor (alpha = 0.001 y epochs = 5000) en cuanto a precisión, el rendimiento del modelo cuando use estos parámetros no fue mejor que utilizando otros hiperparámetros que la función no sugería, probablemente debido a un error en la implementación. Decidí utilizar alpha = 0.01 y epochs = 10000 o 5000, viendo ninguna diferencia variando estos epochs y así el modelo se mostraba mejor. Considero que es un área de oportunidad y un aprendizaje importante obtenido al realizar la práctica, pues concluí que la selección de hiperparámetros es un proceso de experiencia, prueba y error.

En conclusión, se pudo aplicar la regresión logística aprendida en clase y empleada en diversas actividades en un dataset nuevo, en este caso para predecir si un cáncer es benigno o maligno, se utilizaron procesos de escalamiento de features y una correcta división de los datos para asegurar un entrenamiento correcto. Se evaluó el modelo y con la matriz de confusión se mostró que predice bien en la mayoría de casos.

---





"""