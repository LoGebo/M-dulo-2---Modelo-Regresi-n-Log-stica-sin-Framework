{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H7_u1qYoTQO"
      },
      "source": [
        "## Jesús Daniel Martínez García A00833591"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd-cucVNngAC",
        "outputId": "0dc46443-4417-4fbc-a803-d506994623f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Import drive to connect and interact with Google Drive (so we can import the data)\n",
        "# Note: This may take a while, but remember to give permission\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive\")\n",
        "!pwd # Print working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBTr_WkbpGLU",
        "outputId": "c3e0d8a7-b1bc-457c-cf68-e667072428a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/IA/NOTEBOOKS\n",
            " bankloan.csv\t\t\t\t'sesion5_actividad5_over_under_fitting_V02 (1).ipynb'\n",
            " Cancer_Data.csv\t\t\t TC3006C.M3.01.ETL.pdf\n",
            " evidencia1.ipynb\t\t\t TC3006C.M3.02.Tutorial.Numpy.ipynb\n",
            " hypothesis_function_alumno.ipynb\t TC3006C_M3_03_Tutorial_Pandas.ipynb\n",
            " iris.data\t\t\t\t TC3006C.M3.04.Tutorial.Scipy.ipynb\n",
            "'linear_reg_gd1_alumno (1).ipynb'\t TC3006C_M3_05_Tutorial_Scikit_learn.ipynb\n",
            "'log_reg_gd_V06_alumno (1).ipynb'\t TC3006C_M3_07_Tutorial_Seaborn.ipynb\n",
            "'log_reg_multiclase_alumnos (2).ipynb'\t wine.data\n",
            "'perceptron_and_or_xnor (1) (1).ipynb'\t wine.names\n",
            " score_updated.csv\n"
          ]
        }
      ],
      "source": [
        "# Navigate to the path where the dataset is stored and read the csv file\n",
        "%cd \"/content/gdrive/MyDrive/IA/NOTEBOOKS\"\n",
        "!ls # List files located in defined folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "id": "XAfl5w5opMHy",
        "outputId": "54276f1e-903e-4139-bd00-d37ec9558124"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>...</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>...</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>...</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>...</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>...</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>...</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
              "0      842302         M        17.99         10.38          122.80     1001.0   \n",
              "1      842517         M        20.57         17.77          132.90     1326.0   \n",
              "2    84300903         M        19.69         21.25          130.00     1203.0   \n",
              "3    84348301         M        11.42         20.38           77.58      386.1   \n",
              "4    84358402         M        20.29         14.34          135.10     1297.0   \n",
              "..        ...       ...          ...           ...             ...        ...   \n",
              "564    926424         M        21.56         22.39          142.00     1479.0   \n",
              "565    926682         M        20.13         28.25          131.20     1261.0   \n",
              "566    926954         M        16.60         28.08          108.30      858.1   \n",
              "567    927241         M        20.60         29.33          140.10     1265.0   \n",
              "568     92751         B         7.76         24.54           47.92      181.0   \n",
              "\n",
              "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
              "0            0.11840           0.27760         0.30010              0.14710   \n",
              "1            0.08474           0.07864         0.08690              0.07017   \n",
              "2            0.10960           0.15990         0.19740              0.12790   \n",
              "3            0.14250           0.28390         0.24140              0.10520   \n",
              "4            0.10030           0.13280         0.19800              0.10430   \n",
              "..               ...               ...             ...                  ...   \n",
              "564          0.11100           0.11590         0.24390              0.13890   \n",
              "565          0.09780           0.10340         0.14400              0.09791   \n",
              "566          0.08455           0.10230         0.09251              0.05302   \n",
              "567          0.11780           0.27700         0.35140              0.15200   \n",
              "568          0.05263           0.04362         0.00000              0.00000   \n",
              "\n",
              "     ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
              "0    ...          17.33           184.60      2019.0           0.16220   \n",
              "1    ...          23.41           158.80      1956.0           0.12380   \n",
              "2    ...          25.53           152.50      1709.0           0.14440   \n",
              "3    ...          26.50            98.87       567.7           0.20980   \n",
              "4    ...          16.67           152.20      1575.0           0.13740   \n",
              "..   ...            ...              ...         ...               ...   \n",
              "564  ...          26.40           166.10      2027.0           0.14100   \n",
              "565  ...          38.25           155.00      1731.0           0.11660   \n",
              "566  ...          34.12           126.70      1124.0           0.11390   \n",
              "567  ...          39.42           184.60      1821.0           0.16500   \n",
              "568  ...          30.37            59.16       268.6           0.08996   \n",
              "\n",
              "     compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
              "0              0.66560           0.7119                0.2654          0.4601   \n",
              "1              0.18660           0.2416                0.1860          0.2750   \n",
              "2              0.42450           0.4504                0.2430          0.3613   \n",
              "3              0.86630           0.6869                0.2575          0.6638   \n",
              "4              0.20500           0.4000                0.1625          0.2364   \n",
              "..                 ...              ...                   ...             ...   \n",
              "564            0.21130           0.4107                0.2216          0.2060   \n",
              "565            0.19220           0.3215                0.1628          0.2572   \n",
              "566            0.30940           0.3403                0.1418          0.2218   \n",
              "567            0.86810           0.9387                0.2650          0.4087   \n",
              "568            0.06444           0.0000                0.0000          0.2871   \n",
              "\n",
              "     fractal_dimension_worst  Unnamed: 32  \n",
              "0                    0.11890          NaN  \n",
              "1                    0.08902          NaN  \n",
              "2                    0.08758          NaN  \n",
              "3                    0.17300          NaN  \n",
              "4                    0.07678          NaN  \n",
              "..                       ...          ...  \n",
              "564                  0.07115          NaN  \n",
              "565                  0.06637          NaN  \n",
              "566                  0.07820          NaN  \n",
              "567                  0.12400          NaN  \n",
              "568                  0.07039          NaN  \n",
              "\n",
              "[569 rows x 33 columns]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# dataset de kaggle https://www.kaggle.com/datasets/vikramamin/bank-loan-approval-lr-dt-rf-and-auc\n",
        "df = pd.read_csv('Cancer_Data.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "I-LoJL5OpZHj"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Convertimos  variable categorica para que el modelo pueda manejarla.\n",
        "df['diagnosis'] = df['diagnosis'].replace({'B': 0, 'M': 1})\n",
        "\n",
        "df_x = df[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean',\n",
        "           'compactness_mean', 'concavity_mean', 'concave points_mean']]\n",
        "\n",
        "## Nuestra variable a predecir con la regresión logística será el diagnóstico (0 = Benigno, 1 = Maligno)\n",
        "df_y = df['diagnosis']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xC7TPkxVrB5y"
      },
      "outputs": [],
      "source": [
        "## Aquí utilizaremos la función de train_test_split como la utilizada en la actividad de regresión logistica multiclase para separar nuestro datasets en datos de entrenamiento y prueba\n",
        "##la función divide los datos en 80% entrenamiento y 20% prueba.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Esto nos servirá para ver si el modelo generaliza bien con otros datos\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0CySmvv2sMhz"
      },
      "outputs": [],
      "source": [
        "## Escalar las features nos permitira que el modelo trate a todas las features con igual de importancia\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train_scaled = sc.fit_transform(X_train)\n",
        "\n",
        "\n",
        "#transform utilizda en los datos de prueba\n",
        "X_test_scaled = sc.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Om9FHW0wDVr",
        "outputId": "2e9c4bda-2064-42df-868a-25d31107d49c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.         -1.44075296 -0.43531947 -1.36208497 -1.1391179   0.78057331\n",
            "   0.71892128  2.82313451 -0.11914956]\n",
            " [ 1.          1.97409619  1.73302577  2.09167167  1.85197292  1.319843\n",
            "   3.42627493  2.01311199  2.66503199]\n",
            " [ 1.         -1.39998202 -1.24962228 -1.34520926 -1.10978518 -1.33264483\n",
            "  -0.30735463 -0.36555756 -0.69650228]\n",
            " [ 1.         -0.98179678  1.41622208 -0.98258746 -0.86694414  0.05938999\n",
            "  -0.59678772 -0.82020317 -0.84511471]\n",
            " [ 1.         -1.11769991 -1.0102595  -1.12500192 -0.96594206  1.26951116\n",
            "  -0.43900185 -0.98334145 -0.93059974]]\n",
            "(455, 9)\n"
          ]
        }
      ],
      "source": [
        "##inicializacion de theta y columna de 1sss\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Inicialización del vector theta con valores random (incluyendo el término dell bias)\n",
        "\n",
        "\n",
        "theta = np.random.randn(len(X_train_scaled[0]) + 1, 1)\n",
        "\n",
        "# Añadir la columna de unos a X_train_scaled para incluir el bias en X_vect\n",
        "X_vect = np.c_[np.ones((len(X_train_scaled), 1)), X_train_scaled]\n",
        "X_test_vect = np.c_[np.ones((len(X_test_scaled), 1)), X_test_scaled]\n",
        "\n",
        "\n",
        "print(X_vect[:5])\n",
        "print(X_vect.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MTDnlSM5tXmq"
      },
      "outputs": [],
      "source": [
        "##DEFINIMOS LAS FUNCIONES NECESARIAS PARA IMEPLEMENTAR LA REGRESIÓN LOGÍSTICA\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "##funcion sigmoide convierte en un num entre 0 y 1\n",
        "def sigmoid_function(X):\n",
        "    return 1 / (1 + np.exp(-X))\n",
        "\n",
        "\n",
        "\n",
        "##Regresión logística, utiliza gradient descent para ajustar las thetas, x son los features, y la label,\n",
        "#alpha el learning reate y los epochs la cantidad de veces que se actulizarán los parametros\n",
        "def log_regression(X, y, theta, alpha, epochs):\n",
        "  y_ = np.reshape(y, (len(y), 1)) # shape (150,1)\n",
        "  N = len(X)\n",
        "  avg_loss_list = []\n",
        "  for epoch in range(epochs):\n",
        "    sigmoid_x_theta = sigmoid_function(X_vect.dot(theta)) # shape: (150,5).(5,1) = (150,1)\n",
        "    grad = (1/N) * X_vect.T.dot(sigmoid_x_theta - y_) # shapes: (5,150).(150,1) = (5, 1)\n",
        "    theta = theta - (alpha * grad)\n",
        "    hyp = sigmoid_function(X_vect.dot(theta)) # shape (150,5).(5,1) = (150,1)\n",
        "    avg_loss = -np.sum(np.dot(y_.T, np.log(hyp) + np.dot((1-y_).T, np.log(1-hyp)))) / len(hyp)\n",
        "    if epoch % 1000 == 0:\n",
        "      print('epoch: {} | avg_loss: {}'.format(epoch, avg_loss))\n",
        "\n",
        "    avg_loss_list.append(avg_loss)\n",
        "  plt.plot(np.arange(1, epochs), avg_loss_list[1:], color='red')\n",
        "  plt.title('Cost function')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Cost')\n",
        "  plt.show()\n",
        "  return theta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "M_MaM0VmKGXu",
        "outputId": "d9f373e0-f169-4f64-cf3d-b7e8f570a5ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0 | avg_loss: 160.78518868520163\n",
            "epoch: 1000 | avg_loss: 30.879753090433926\n",
            "epoch: 2000 | avg_loss: 20.545961872651784\n",
            "epoch: 3000 | avg_loss: 16.674093161543798\n",
            "epoch: 4000 | avg_loss: 14.77269821993411\n",
            "epoch: 5000 | avg_loss: 13.66235349119574\n",
            "epoch: 6000 | avg_loss: 12.929316706912438\n",
            "epoch: 7000 | avg_loss: 12.405610471047133\n",
            "epoch: 8000 | avg_loss: 12.012424051663658\n",
            "epoch: 9000 | avg_loss: 11.707349536247623\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Entrenar el modelo\n",
        "\n",
        "alpha = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "theta_final = log_regression(X_vect, y_train, theta, alpha, epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FdGOnuPxMNf",
        "outputId": "6e98b641-103c-4def-e22b-12d25c493456"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión del modelo: 95.61%\n",
            "Predicted   0   1\n",
            "Actual           \n",
            "0          69   2\n",
            "1           3  40\n",
            "Exactitud calculada.   : 95.61%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def predict(X, theta):\n",
        "    probabilities = sigmoid_function(np.dot(X, theta))\n",
        "    return probabilities >= 0.5  # Si la probabilidad es mayor o igual a 0.5, predice 1, sino nel 0\n",
        "\n",
        "\n",
        "\n",
        "# Asegurarnos de q theta_final tiene el tipo de dato correcto\n",
        "theta_final = theta_final.astype(float)\n",
        "\n",
        "# Hacer predicciones con el conjunto de prueba\n",
        "y_pred_test = predict(X_test_vect, theta_final)\n",
        "\n",
        "# Convertir las predicciones booleanas en 0 y 1 para calcular las métricas\n",
        "y_pred_test = y_pred_test.astype(int)\n",
        "\n",
        "# Aplanar los arrays si tibnes multidimension\n",
        "y_test = np.array(y_test).ravel()\n",
        "y_pred_test = np.array(y_pred_test).ravel()\n",
        "\n",
        "# Calcular la precisión manualmente\n",
        "accuracy_manual = np.mean(y_test == y_pred_test)\n",
        "print(f'Precisión del modelo: {accuracy_manual * 100:.2f}%')\n",
        "\n",
        "\n",
        "\n",
        "# Usa pandas para generar la matriz de confusión\n",
        "conf_matrix = pd.crosstab(pd.Series(y_test, name='Actual'), pd.Series(y_pred_test, name='Predicted'))\n",
        "\n",
        "\n",
        "print(conf_matrix)\n",
        "\n",
        "\n",
        "accuracy = np.mean(y_test == y_pred_test)\n",
        "\n",
        "\n",
        "print(f'Exactitud calculada.   : {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMcBFkpQ7BLk"
      },
      "source": [
        "##Reporte.\n",
        "\n",
        "Para el dataset de cáncer data buscamos predecir el label de diagnosis donde m representaba cáncer maligno y b cáncer benigno. Se implementa el modelo de regresión logística al ser un algoritmo efectivo para el problema que representa, viendo que es una clasificación binaria.\n",
        "\n",
        "Primero se cargan los datos utilizando pandas, se separan las features y labels así como transformar la label categórica a numérica con 0 para benigno y 1 para maligno, lo que facilitaría el uso en el modelo.\n",
        "\n",
        "Después dividimos los datos de entrenamiento: 80% training y 20% prueba. Esto lo hacemos para evaluar el modelo con datos que no ha visto antes y validar si generaliza o no.\n",
        "\n",
        " Se aplicó un standar scaler, que básicamente asegura que todas las features tengan el mismo peso en el entrenamiento y para el de prueba se escalan usando lo mismo que se calculó en los de entrenamiento, uno con fit_transform y el otro solo con transform.\n",
        "\n",
        " En las funciones relevantes para la regresión logística se definió la sigmoide y la función para entrenar el modelo utilizando el gradient descent que ajusta las thetas, alpha sería el learning rate, que contra que tran rapido se ajustan las thetas en cada iteración, después los epochs que es la cantidad de veces que se van a actualizar los parametros del modelo.\n",
        "\n",
        " Se crea una función predict que utiliza la funcion sigmoide que calcule las probabilidades de que un diagnotico sea maligno (1), si es mayor o igual a 0.5, si es menor, predice benigno (0).\n",
        "\n",
        " Se hacen las predicciones sobre los datos de test y se comparan. El accuracy ahí nos da cuántas veces el modelo predijo correctamente, en este caso obtuvo un 96.4%. También, se implementó una matriz de confusión que muestra cómo se comportó el modelo con más detalle: verdaderos negativos (70), es decir, que el modelo predijo bien que 70 personas no tenían cáncer maligno. Falsos positivos (1) el modelo solo predijo mal que una persona tenía cáncer maligno, cuando no. Verdaderos positivos (40), el modelo predijo correctamente que esa cantidad tenía cáncer maligno. Falsos negativos (3), el modelo dio mal que 3 personas no tenían cáncer maligno, cuando sí.\n",
        "\n",
        "En cuanto a los hiperparámetros, se decidió utilizar la función de grid search, específicamente para ajustar el learning rate y los epochs, durante estas pruebas aunque la función si identificaba una combinación como la mejor (alpha = 0.001 y epochs = 5000) en cuanto a precisión, el rendimiento del modelo cuando use estos parámetros no fue mejor que utilizando otros hiperparámetros que la función no sugería, probablemente debido a un error en la implementación. Decidí utilizar alpha = 0.01 y epochs = 10000 o 5000, viendo ninguna diferencia variando estos epochs y así el modelo se mostraba mejor. Considero que es un área de oportunidad y un aprendizaje importante obtenido al realizar la práctica, pues concluí que la selección de hiperparámetros es un proceso de experiencia, prueba y error.\n",
        "\n",
        "En conclusión, se pudo aplicar la regresión logística aprendida en clase y empleada en diversas actividades en un dataset nuevo, en este caso para predecir si un cáncer es benigno o maligno, se utilizaron procesos de escalamiento de features y una correcta división de los datos para asegurar un entrenamiento correcto. Se evaluó el modelo y con la matriz de confusión se mostró que predice bien en la mayoría de casos.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
